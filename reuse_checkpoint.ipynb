{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from depth_anything.dpt import DepthAnything\n",
    "from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DepthAnything.from_pretrained(\"LiheYoung/depth_anything_vits14\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.models.vision_transformer import vit_small, vit_base, vit_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vits = vit_small(patch_size=14, init_values=1e-5, img_size=518)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vits_item in vits.state_dict():\n",
    "    vits_item_inmodel = vits_item.replace('blocks.0.', 'blocks.')\n",
    "    if vits_item_inmodel in model.pretrained.state_dict() and vits.state_dict()[vits_item].size() == model.pretrained.state_dict()[vits_item_inmodel].size():\n",
    "        continue\n",
    "    else:\n",
    "        print(vits_item, vits.state_dict()[vits_item].size(), model.pretrained.state_dict()[vits_item_inmodel].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_item in model.pretrained.state_dict():\n",
    "    model_item_invit = model_item.replace('blocks.', 'blocks.0.')\n",
    "    if model_item_invit in vits.state_dict() and vits.state_dict()[model_item_invit].size() == model.pretrained.state_dict()[model_item].size():\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            print(model_item, vits.state_dict()[model_item_invit].size(), model.pretrained.state_dict()[model_item].size())\n",
    "        except:\n",
    "            print(f'{model_item} not in VitS!', model.pretrained.state_dict()[model_item].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token torch.Size([1, 1, 384])\n",
      "pos_embed torch.Size([1, 1370, 384])\n",
      "mask_token torch.Size([1, 384])\n",
      "patch_embed.proj.weight torch.Size([384, 3, 14, 14])\n",
      "patch_embed.proj.bias torch.Size([384])\n",
      "blocks.0.0.norm1.weight torch.Size([384])\n",
      "blocks.0.0.norm1.bias torch.Size([384])\n",
      "blocks.0.0.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.0.0.attn.qkv.bias torch.Size([1152])\n",
      "blocks.0.0.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.0.0.attn.proj.bias torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "pretrained_vit = OrderedDict()\n",
    "for model_item in model.pretrained.state_dict():\n",
    "    model_item_invit = model_item.replace('blocks.', 'blocks.0.')\n",
    "    pretrained_vit[model_item_invit] = model.pretrained.state_dict()[model_item]\n",
    "torch.save(pretrained_vit, 'checkpoints/vits.pth')\n",
    "\n",
    "for i, item in enumerate(pretrained_vit):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(item, pretrained_vit[item].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pthfile = 'checkpoints/vits.pth'\n",
    "vits.load_state_dict(torch.load(pthfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitb = vit_base(patch_size=14, init_values=1e-5, img_size=518)\n",
    "\n",
    "model = DepthAnything.from_pretrained(\"checkpoints/depth_anything_vitb14\", local_files_only=True)\n",
    "\n",
    "pretrained_vit = OrderedDict()\n",
    "for model_item in model.pretrained.state_dict():\n",
    "    model_item_invit = model_item.replace('blocks.', 'blocks.0.')\n",
    "    pretrained_vit[model_item_invit] = model.pretrained.state_dict()[model_item]\n",
    "torch.save(pretrained_vit, 'checkpoints/vitb.pth')\n",
    "\n",
    "pthfile = 'checkpoints/vitb.pth'\n",
    "vitb.load_state_dict(torch.load(pthfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitl = vit_large(patch_size=14, init_values=1e-5, img_size=518)\n",
    "\n",
    "model = DepthAnything.from_pretrained(\"checkpoints/depth_anything_vitl14\", local_files_only=True)\n",
    "\n",
    "pretrained_vit = OrderedDict()\n",
    "for model_item in model.pretrained.state_dict():\n",
    "    model_item_invit = model_item.replace('blocks.', 'blocks.0.')\n",
    "    pretrained_vit[model_item_invit] = model.pretrained.state_dict()[model_item]\n",
    "torch.save(pretrained_vit, 'checkpoints/vitl.pth')\n",
    "\n",
    "pthfile = 'checkpoints/vitl.pth'\n",
    "vitl.load_state_dict(torch.load(pthfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
